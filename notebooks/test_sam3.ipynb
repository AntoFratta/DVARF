{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "notebook_title"
            },
            "source": [
                "# SAM 3 Zero-Shot Object Detection on DVARF Dataset\n",
                "\n",
                "This notebook demonstrates the zero-shot object detection capabilities of Meta's Segment Anything Model 3 (SAM 3) on the DVARF dataset for accident scene analysis from drone imagery."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup_header"
            },
            "source": [
                "## 1. Environment Setup\n",
                "\n",
                "Clone the repository and navigate to project directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "clone_repo"
            },
            "outputs": [],
            "source": [
                "%cd /content\n",
                "\n",
                "# Clone the DVARF repository\n",
                "!git clone https://github.com/AntoFratta/DVARF.git\n",
                "\n",
                "%cd /content/DVARF"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "install_header"
            },
            "source": [
                "## 2. Install Dependencies\n",
                "\n",
                "Install project dependencies. Note that we filter out Windows-specific packages (e.g., `triton-windows`) which are not compatible with Linux-based Colab environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "# Create a Colab-compatible requirements file by removing Windows-specific packages\n",
                "!grep -v \"triton-windows\" requirements.txt > requirements_colab.txt\n",
                "\n",
                "# Install all dependencies from the filtered requirements file\n",
                "!pip install -r requirements_colab.txt\n",
                "\n",
                "# Install triton for Linux\n",
                "!pip install triton"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "sam3_header"
            },
            "source": [
                "## 4. Install SAM 3\n",
                "\n",
                "Clone and install the official SAM 3 repository from Meta."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "clone_sam3"
            },
            "outputs": [],
            "source": [
                "%cd /content\n",
                "\n",
                "# Clone the official SAM 3 repository\n",
                "!git clone https://github.com/facebookresearch/sam3.git"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_sam3"
            },
            "outputs": [],
            "source": [
                "%cd /content/sam3\n",
                "\n",
                "# Install SAM 3 in editable mode\n",
                "!pip install -e ."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "hf_header"
            },
            "source": [
                "## 5. Hugging Face Authentication\n",
                "\n",
                "Authenticate with Hugging Face to access the SAM 3 model weights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "hf_login"
            },
            "outputs": [],
            "source": [
                "from huggingface_hub import login\n",
                "\n",
                "# Login to Hugging Face (a widget will appear to enter your token)\n",
                "login()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "inference_header"
            },
            "source": [
                "## 6. Run SAM 3 on Test Split\n",
                "\n",
                "Execute zero-shot inference using SAM 3 on the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "run_inference"
            },
            "outputs": [],
            "source": [
                "%cd /content/DVARF\n",
                "\n",
                "import sys\n",
                "if \"/content/DVARF\" not in sys.path:\n",
                "    sys.path.insert(0, \"/content/DVARF\")\n",
                "\n",
                "from scripts.run_sam3_on_split import run_sam3_on_split\n",
                "\n",
                "# Run SAM 3 zero-shot inference on the test split\n",
                "# Set save_segmentations=True to save segmentation masks for visualization\n",
                "run_sam3_on_split(\n",
                "    split=\"test\",\n",
                "    score_threshold=0.26,\n",
                "    max_images=None,\n",
                "    save_segmentations=True,\n",
                "    max_masks_per_image_per_class=None,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "eval_header"
            },
            "source": [
                "## 7. Evaluate Results\n",
                "\n",
                "Compute precision, recall, F1-score, and mean Average Precision (mAP) on the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "evaluate"
            },
            "outputs": [],
            "source": [
                "%cd /content/DVARF\n",
                "\n",
                "# Evaluate SAM 3 predictions and save metrics to results folder\n",
                "!python scripts/eval_sam3_on_split.py | tee results/sam3_test_metrics.txt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "additional_metrics_header"
            },
            "source": [
                "## 8. Calcolo metriche aggiuntive per confronto con la tesi\n",
                "\n",
                "Calculate additional metrics for comparison with the thesis:\n",
                "- **Class-specific average IoU**: Mean IoU for True Positives (predictions correctly matched to ground truth with IoU ≥ 0.5)\n",
                "- **Inference speed**: Average inference time per frame (model forward pass + post-processing only, excluding I/O)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "calculate_iou_metrics"
            },
            "outputs": [],
            "source": [
                "%cd /content/DVARF\n",
                "\n",
                "import sys\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from time import time\n",
                "from PIL import Image\n",
                "\n",
                "if \"/content/DVARF\" not in sys.path:\n",
                "    sys.path.insert(0, \"/content/DVARF\")\n",
                "\n",
                "from src.config import get_labels_dir, get_sam3_yolo_predictions_dir, get_images_dir\n",
                "from src.prompts import CLASS_PROMPTS\n",
                "from src.eval_yolo import _load_yolo_dataset, _compute_iou, _yolo_to_xyxy\n",
                "\n",
                "# ============================================================================\n",
                "# PART 1: Calculate class-specific average IoU\n",
                "# ============================================================================\n",
                "\n",
                "split = \"test\"\n",
                "confidence_threshold = 0.26\n",
                "iou_threshold = 0.5\n",
                "num_classes = len(CLASS_PROMPTS)\n",
                "\n",
                "labels_dir = get_labels_dir(split)\n",
                "preds_dir = get_sam3_yolo_predictions_dir(split)\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"CLASS-SPECIFIC AVERAGE IoU (True Positives with IoU ≥ 0.5)\")\n",
                "print(\"=\"*70)\n",
                "print(f\"Split: {split}\")\n",
                "print(f\"Labels directory: {labels_dir}\")\n",
                "print(f\"Predictions directory: {preds_dir}\")\n",
                "print(f\"Confidence threshold: {confidence_threshold}\")\n",
                "print(f\"IoU threshold: {iou_threshold}\\n\")\n",
                "\n",
                "# Load ground truth and predictions\n",
                "gt_by_class, preds_by_class = _load_yolo_dataset(\n",
                "    labels_dir=labels_dir,\n",
                "    preds_dir=preds_dir,\n",
                "    num_classes=num_classes,\n",
                "    confidence_threshold=confidence_threshold,\n",
                ")\n",
                "\n",
                "# Calculate IoU for True Positives per class\n",
                "class_names = {cid: name for cid, name in CLASS_PROMPTS.items()}\n",
                "iou_per_class = {}\n",
                "\n",
                "for c in range(num_classes):\n",
                "    gt_dict = gt_by_class.get(c, {})\n",
                "    preds = preds_by_class.get(c, [])\n",
                "    \n",
                "    if len(preds) == 0:\n",
                "        iou_per_class[c] = []\n",
                "        continue\n",
                "    \n",
                "    # Sort predictions by score (highest first)\n",
                "    preds_sorted = sorted(preds, key=lambda x: x[2], reverse=True)\n",
                "    \n",
                "    # Track which GT boxes have been matched\n",
                "    matched = {}\n",
                "    for img_id, boxes in gt_dict.items():\n",
                "        matched[img_id] = np.zeros(len(boxes), dtype=bool)\n",
                "    \n",
                "    # Collect IoU values for True Positives\n",
                "    tp_ious = []\n",
                "    \n",
                "    for img_id, box_pred, _score in preds_sorted:\n",
                "        gt_boxes = gt_dict.get(img_id)\n",
                "        if gt_boxes is None or gt_boxes.size == 0:\n",
                "            continue  # False Positive (no GT in this image)\n",
                "        \n",
                "        # Compute IoU with all GT boxes in this image\n",
                "        ious = _compute_iou(box_pred, gt_boxes)\n",
                "        best_idx = int(np.argmax(ious))\n",
                "        best_iou = float(ious[best_idx])\n",
                "        \n",
                "        # True Positive if IoU ≥ threshold and GT not already matched\n",
                "        if best_iou >= iou_threshold and not matched[img_id][best_idx]:\n",
                "            tp_ious.append(best_iou)\n",
                "            matched[img_id][best_idx] = True\n",
                "    \n",
                "    iou_per_class[c] = tp_ious\n",
                "\n",
                "# Calculate and display mean IoU per class\n",
                "mean_ious = {}\n",
                "for c in range(num_classes):\n",
                "    class_name = class_names.get(c, f\"class_{c}\")\n",
                "    ious = iou_per_class[c]\n",
                "    \n",
                "    if len(ious) > 0:\n",
                "        mean_iou = np.mean(ious)\n",
                "        mean_ious[c] = mean_iou\n",
                "        print(f\"  {class_name:15s}: IoU_medio = {mean_iou:.4f}  (n_TP = {len(ious)})\")\n",
                "    else:\n",
                "        mean_ious[c] = 0.0\n",
                "        print(f\"  {class_name:15s}: IoU_medio = 0.0000  (n_TP = 0)\")\n",
                "\n",
                "# Calculate overall mean IoU\n",
                "if len(mean_ious) > 0:\n",
                "    iou_medio_totale = np.mean(list(mean_ious.values()))\n",
                "    print(f\"\\n  {'MEAN (all classes)':15s}: IoU_medio_totale = {iou_medio_totale:.4f}\")\n",
                "else:\n",
                "    iou_medio_totale = 0.0\n",
                "    print(f\"\\n  {'MEAN (all classes)':15s}: IoU_medio_totale = 0.0000\")\n",
                "\n",
                "\n",
                "# Save metrics to file\n",
                "metrics_file = Path(\"results/sam3_test_metrics.txt\")\n",
                "with open(metrics_file, \"a\", encoding=\"utf-8\") as f:\n",
                "    f.write(\"\\n\\n\" + \"=\"*70 + \"\\n\")\n",
                "    f.write(\"CLASS-SPECIFIC AVERAGE IoU (True Positives with IoU ≥ 0.5)\\n\")\n",
                "    f.write(\"=\"*70 + \"\\n\")\n",
                "    for c in range(num_classes):\n",
                "        class_name = class_names.get(c, f\"class_{c}\")\n",
                "        ious = iou_per_class[c]\n",
                "        if len(ious) > 0:\n",
                "            mean_iou = mean_ious[c]\n",
                "            f.write(f\"  {class_name:15s}: IoU_medio = {mean_iou:.4f}  (n_TP = {len(ious)})\\n\")\n",
                "        else:\n",
                "            f.write(f\"  {class_name:15s}: IoU_medio = 0.0000  (n_TP = 0)\\n\")\n",
                "    f.write(f\"\\n  {'MEAN (all classes)':15s}: IoU_medio_totale = {iou_medio_totale:.4f}\\n\")\n",
                "    f.write(\"=\"*70 + \"\\n\")\n",
                "\n",
                "print(f\"\\nMetrics saved to: {metrics_file}\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "measure_speed"
            },
            "outputs": [],
            "source": [
                "%cd /content/DVARF\n",
                "\n",
                "import sys\n",
                "import torch\n",
                "from time import time\n",
                "from pathlib import Path\n",
                "\n",
                "if \"/content/DVARF\" not in sys.path:\n",
                "    sys.path.insert(0, \"/content/DVARF\")\n",
                "\n",
                "from src.config import get_images_dir\n",
                "from src.prompts import CLASS_PROMPTS\n",
                "from src.sam3_wrapper import Sam3ImageModel\n",
                "from src.yolo_export import sam3_boxes_to_yolo, nms_yolo_boxes\n",
                "\n",
                "# ======================================================================\n",
                "# PART 2: Measure inference speed per frame (SAM 3)\n",
                "# ======================================================================\n",
                "# NOTE: The timing includes image loading from disk because SAM 3's\n",
                "# predict_with_text() method loads images internally from file paths.\n",
                "# The measured time represents the complete pipeline: I/O + inference + post-processing.\n",
                "# ======================================================================\n",
                "\n",
                "split = \"test\"\n",
                "score_threshold = 0.26\n",
                "nms_iou = 0.7\n",
                "nms_max_det = 300\n",
                "\n",
                "images_dir = get_images_dir(split)\n",
                "image_files = sorted(\n",
                "    list(images_dir.glob(\"*.jpg\")) + list(images_dir.glob(\"*.png\")),\n",
                "    key=lambda p: int(p.stem),\n",
                ")\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"INFERENCE SPEED MEASUREMENT\")\n",
                "print(\"=\"*70)\n",
                "print(f\"Split: {split}\")\n",
                "print(f\"Number of images: {len(image_files)}\")\n",
                "print(f\"Images directory: {images_dir}\")\n",
                "print(f\"Score threshold: {score_threshold}\")\n",
                "print(f\"NMS: iou={nms_iou}, max_det={nms_max_det}\")\n",
                "print(f\"NOTE: Timing includes I/O + model inference + post-processing\\n\")\n",
                "\n",
                "# Initialize model\n",
                "model = Sam3ImageModel()\n",
                "\n",
                "# Synchronize GPU if available\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.synchronize()\n",
                "\n",
                "# Start timing (I/O + model inference + post-processing)\n",
                "print(\"Starting inference timing...\")\n",
                "t_start = time()\n",
                "\n",
                "for img_path in image_files:\n",
                "    # Get image dimensions (quick read just for metadata)\n",
                "    from PIL import Image\n",
                "    with Image.open(img_path) as img:\n",
                "        width, height = img.size\n",
                "    \n",
                "    all_boxes = []\n",
                "    \n",
                "    # Query SAM 3 for each class (this includes image loading)\n",
                "    for class_id, prompt in CLASS_PROMPTS.items():\n",
                "        prediction = model.predict_with_text(img_path, prompt)\n",
                "        \n",
                "        # Convert to YOLO format\n",
                "        yolo_boxes = sam3_boxes_to_yolo(\n",
                "            prediction=prediction,\n",
                "            class_id=class_id,\n",
                "            image_width=width,\n",
                "            image_height=height,\n",
                "            score_threshold=score_threshold,\n",
                "        )\n",
                "        all_boxes.extend(yolo_boxes)\n",
                "    \n",
                "    # Apply NMS\n",
                "    all_boxes = nms_yolo_boxes(all_boxes, iou_threshold=nms_iou, max_det=nms_max_det)\n",
                "\n",
                "# Synchronize GPU if available\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.synchronize()\n",
                "\n",
                "t_end = time()\n",
                "total_time = t_end - t_start\n",
                "\n",
                "# Calculate speed per frame\n",
                "num_images = len(image_files)\n",
                "speed_per_frame_s = total_time / num_images\n",
                "speed_per_frame_ms = speed_per_frame_s * 1000\n",
                "\n",
                "print(f\"\\nInference completed.\")\n",
                "print(f\"Total time: {total_time:.2f} s\")\n",
                "print(f\"Number of images: {num_images}\")\n",
                "print(f\"Speed per frame: {speed_per_frame_ms:.2f} ms/frame ({speed_per_frame_s:.4f} s/frame)\")\n",
                "\n",
                "# Save speed metrics to file\n",
                "metrics_file = Path(\"results/sam3_test_metrics.txt\")\n",
                "with open(metrics_file, \"a\", encoding=\"utf-8\") as f:\n",
                "    f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
                "    f.write(\"INFERENCE SPEED MEASUREMENT\\n\")\n",
                "    f.write(\"=\"*70 + \"\\n\")\n",
                "    f.write(f\"Measured on: test set ({num_images} images)\\n\")\n",
                "    f.write(f\"Components: I/O + model inference + post-processing (NMS)\\n\")\n",
                "    f.write(f\"Total time: {total_time:.2f} s\\n\")\n",
                "    f.write(f\"Speed per frame: {speed_per_frame_ms:.2f} ms/frame ({speed_per_frame_s:.4f} s/frame)\\n\")\n",
                "    f.write(\"=\"*70 + \"\\n\")\n",
                "\n",
                "print(f\"\\nSpeed metrics saved to: {metrics_file}\")\n",
                "print(\"=\"*70)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
