"""
Lightweight wrapper around the official SAM 3 image model.

This module centralizes all interactions with the SAM 3 API so that the rest
of the project does not depend on the low-level interface of the original
repository. This makes the code easier to maintain and test.
"""

from __future__ import annotations

import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Union, Optional

from PIL import Image
import torch

from sam3.model_builder import build_sam3_image_model
from sam3.model.sam3_image_processor import Sam3Processor


PathLike = Union[str, Path]


@dataclass
class Sam3Prediction:
    """Masks, bounding boxes and scores returned by SAM 3."""
    masks: Any      # usually a tensor of shape [N, 1, H, W]
    boxes: Any      # usually a tensor of shape [N, 4] in pixel coordinates
    scores: Any     # usually a tensor of shape [N]
    features: Any   # per-box semantic features (N, 256)


class Sam3ImageModel:
    """Wrapper for single-image inference with a text prompt."""

    def __init__(self, checkpoint_path: Optional[str] = None) -> None:
        """Build the SAM 3 image model and its processor."""
        # Explicitly load from HuggingFace to avoid bpe_path issues
        # NOTE: We also pass bpe_path explicitly to be robust in Colab/runtime environments.
        import sam3  # local import to avoid issues before dependencies are ready
        bpe_path = str(Path(sam3.__file__).resolve().parent / "assets" / "bpe_simple_vocab_16e6.txt.gz")

        if checkpoint_path is not None:
            # Load from explicit checkpoint path
            self.model = build_sam3_image_model(checkpoint_path=checkpoint_path, bpe_path=bpe_path)
        else:
            # Load from HuggingFace
            self.model = build_sam3_image_model(load_from_HF=True, bpe_path=bpe_path)
        self.processor = Sam3Processor(self.model)

    @staticmethod
    def _pick_feature_map(backbone_out: Dict[str, Any]) -> torch.Tensor:
        """
        Pick a reasonable vision feature map for ROI pooling.
        Priority:
          1) backbone_fpn[0] (highest resolution pyramid feature)
          2) vision_features
          3) sam2_backbone_out (if tensor-like)
        Returns a tensor shaped [1, C, H, W].
        """
        cand = None

        if isinstance(backbone_out.get("backbone_fpn", None), (list, tuple)) and backbone_out["backbone_fpn"]:
            cand = backbone_out["backbone_fpn"][0]
        elif isinstance(backbone_out.get("vision_features", None), (list, tuple)) and backbone_out["vision_features"]:
            cand = backbone_out["vision_features"][0]
        elif isinstance(backbone_out.get("vision_features", None), torch.Tensor):
            cand = backbone_out["vision_features"]
        elif isinstance(backbone_out.get("sam2_backbone_out", None), torch.Tensor):
            cand = backbone_out["sam2_backbone_out"]

        if cand is None or not isinstance(cand, torch.Tensor):
            raise RuntimeError(
                "Could not find a usable vision feature map inside output['backbone_out']. "
                f"Available keys: {list(backbone_out.keys())}"
            )

        # Normalize shape to [1, C, H, W]
        if cand.ndim == 3:
            cand = cand.unsqueeze(0)
        elif cand.ndim != 4:
            raise RuntimeError(f"Unexpected feature map shape {tuple(cand.shape)}; expected 3D or 4D tensor.")

        return cand

    @staticmethod
    def _prompt_vector(backbone_out: Dict[str, Any], device: torch.device, dtype: torch.dtype) -> torch.Tensor:
        """
        Build a 256-d prompt-conditioned vector from language embeddings.
        We use mean pooling across tokens and broadcast it to boxes.
        """
        lang = backbone_out.get("language_embeds", None)
        if not isinstance(lang, torch.Tensor):
            # Fallback: zero vector (still works, just less prompt-conditioned)
            return torch.zeros((256,), device=device, dtype=dtype)

        # lang can be [1, T, 256] or [T, 256] or [256]
        if lang.ndim == 3 and lang.shape[0] == 1:
            lang = lang[0]  # [T, 256]
        if lang.ndim == 2:
            vec = lang.mean(dim=0)  # [256]
        elif lang.ndim == 1:
            vec = lang
        else:
            raise RuntimeError(f"Unexpected language_embeds shape {tuple(lang.shape)}")

        return vec.to(device=device, dtype=dtype)

    @staticmethod
    def _ensure_boxes_pixels(boxes: torch.Tensor, orig_w: int, orig_h: int) -> torch.Tensor:
        """
        Ensure boxes are in pixel XYXY.
        If boxes look normalized (max <= ~1.5), scale to pixels.
        """
        if boxes.numel() == 0:
            return boxes

        mx = float(boxes.max().detach().cpu().item())
        if mx <= 1.5:
            scale = boxes.new_tensor([orig_w, orig_h, orig_w, orig_h])
            return boxes * scale
        return boxes

    def predict_with_text(self, image_path: PathLike, prompt: str) -> Sam3Prediction:
        """
        Run SAM 3 on a single image using a single text prompt.
        """
        image_path = Path(image_path)
        image = Image.open(image_path).convert("RGB")

        debug = os.environ.get("SAM3_DEBUG", "0") == "1"

        state = self.processor.set_image(image)
        output: Dict[str, Any] = self.processor.set_text_prompt(
            state=state,
            prompt=prompt,
        )

        if debug:
            print("TOP KEYS:", list(output.keys()))
            bo = output.get("backbone_out", None)
            print("backbone_out type:", type(bo))
            if isinstance(bo, dict):
                print("BACKBONE_OUT KEYS:", list(bo.keys()))
            if "boxes" in output and hasattr(output["boxes"], "shape"):
                print("boxes shape:", output["boxes"].shape)

        # Get final predictions
        final_masks = output["masks"]
        final_boxes = output["boxes"]
        final_scores = output["scores"]

        # If no detections, return empty features.
        if hasattr(final_boxes, "shape") and final_boxes.shape[0] == 0:
            empty_feats = torch.empty((0, 256), device=final_scores.device, dtype=final_scores.dtype)
            return Sam3Prediction(
                masks=final_masks,
                boxes=final_boxes,
                scores=final_scores,
                features=empty_feats,
            )

        # Build per-box semantic features via ROI pooling on a backbone feature map,
        # then condition them on the prompt using language_embeds.
        backbone_out = output.get("backbone_out", None)
        if not isinstance(backbone_out, dict):
            raise RuntimeError("SAM3 output does not contain 'backbone_out' dict; cannot extract features.")

        feat_map = self._pick_feature_map(backbone_out)  # [1, C, Hf, Wf]
        feat_map = feat_map.to(device=final_boxes.device)

        orig_h = int(output["original_height"])
        orig_w = int(output["original_width"])

        boxes_px = self._ensure_boxes_pixels(final_boxes, orig_w, orig_h)

        # ROIAlign expects boxes with batch indices: (N, 5) -> [batch_idx, x1, y1, x2, y2]
        n = boxes_px.shape[0]
        batch_idx = torch.zeros((n, 1), device=boxes_px.device, dtype=boxes_px.dtype)
        rois = torch.cat([batch_idx, boxes_px], dim=1)

        # Compute spatial_scale: feature_map pixels per input pixel
        # Usually Hf/H == Wf/W; use width-based scale.
        spatial_scale = feat_map.shape[-1] / float(orig_w)

        try:
            from torchvision.ops import roi_align
        except Exception as e:
            raise RuntimeError(
                "torchvision.ops.roi_align is required to extract per-box features. "
                "Ensure torchvision is installed and compatible with your torch build."
            ) from e

        # Pool to 1x1 per ROI -> [N, C, 1, 1] -> [N, C]
        pooled = roi_align(
            input=feat_map,
            boxes=rois,
            output_size=(1, 1),
            spatial_scale=spatial_scale,
            sampling_ratio=-1,
            aligned=True,
        ).flatten(1)

        # Prompt conditioning: add mean-pooled language embedding (256-d) to each ROI feature
        prompt_vec = self._prompt_vector(backbone_out, device=pooled.device, dtype=pooled.dtype)
        if pooled.shape[1] != prompt_vec.shape[0]:
            # If C != 256, we can't keep 256-d features. Fail loudly (avoids silent bugs).
            raise RuntimeError(
                f"Feature map channel dim C={pooled.shape[1]} does not match prompt dim {prompt_vec.shape[0]}. "
                "Pick a different feature map (expected C=256) or change downstream feature_dim."
            )

        features = pooled + prompt_vec.unsqueeze(0)  # [N, 256]

        return Sam3Prediction(
            masks=final_masks,
            boxes=final_boxes,
            scores=final_scores,
            features=features,
        )
